/*
 * This file is part of the Score-P software (http://www.score-p.org)
 *
 * Copyright (c) 2009-2013,
 *    RWTH Aachen University, Germany
 *    Gesellschaft fuer numerische Simulation mbH Braunschweig, Germany
 *    Technische Universitaet Dresden, Germany
 *    University of Oregon, Eugene, USA
 *    Forschungszentrum Juelich GmbH, Germany
 *    German Research School for Simulation Sciences GmbH, Juelich/Aachen, Germany
 *    Technische Universitaet Muenchen, Germany
 *
 * See the COPYING file in the package base directory for details.
 *
 */

/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2011                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  Copyright (c) 2010-2011                                                **
**  German Research School for Simulation Sciences GmbH,                   **
**  Laboratory for Parallel Programming                                    **
**                                                                         **
**  Copyright (c) 2003-2008                                                **
**  University of Tennessee, Innovative Computing Laboratory               **
**                                                                         **
**  See the file COPYRIGHT in the package base directory for details       **
****************************************************************************/


/**
 * @file  SCOREP_Mpi_Coll.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     alpha
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for collective communication
 */

#include <config.h>
#include "SCOREP_Mpi.h"
#include "scorep_mpi_communicator.h"
#include <SCOREP_Events.h>

/**
 * @name C wrappers
 * @{
 */

#if HAVE( DECL_PMPI_ALLGATHER )
/**
 * Measurement wrapper for MPI_Allgather
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            recvsz, sendsz, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Comm_size( comm, &N );
        PMPI_Type_size( recvtype, &recvsz );

     #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
     #endif
        {
            PMPI_Type_size( sendtype, &sendsz );
            sendbytes = N * sendcount * sendsz;
            recvbytes = N * recvcount * recvsz;
        }
     #if HAVE( MPI_IN_PLACE )
        else
        {
            sendbytes = recvbytes = ( N - 1 ) * recvcount * recvsz;
        }
     #endif

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLGATHER ] );

        return_val = PMPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLGATHER ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLGATHER,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLGATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allgatherv )
/**
 * Measurement wrapper for MPI_Allgatherv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t        recvcount, recvsz, sendsz, i, N, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Comm_size( comm, &N );
        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Comm_rank( comm, &me );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            PMPI_Type_size( sendtype, &sendsz );
            sendbytes = N * sendcount * sendsz;
        }
      #if HAVE( MPI_IN_PLACE )
        else
        {
            sendbytes = ( N - 1 ) * recvcounts[ me ] * recvsz;
        }
      #endif

        recvcount = 0;
        for ( i = 0; i < N; i++ )
        {
            recvcount += recvcounts[ i ];
        }

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            recvcount -= recvcounts[ me ];
        }
      #endif
        recvbytes = recvcount * recvsz;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLGATHERV ] );

        return_val = PMPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLGATHERV ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLGATHERV,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLREDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allreduce )
/**
 * Measurement wrapper for MPI_Allreduce
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allreduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t        sz, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_size( comm, &N );

      #if HAVE( HAS_MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            sendbytes = recvbytes = ( N - 1 ) * count * sz;
        }
        else
      #endif
        {
            sendbytes = recvbytes = N * count * sz;
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLREDUCE ] );

        return_val = PMPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLREDUCE ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLREDUCE,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoall )
/**
 * Measurement wrapper for MPI_Alltoall
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoall( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t        recvsz, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Comm_size( comm, &N );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            --N;
        }
      #endif

        sendbytes = N * recvcount * recvsz;
        recvbytes = sendbytes;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLTOALL ] );

        return_val = PMPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLTOALL ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLTOALL,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALLV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallv )
/**
 * Measurement wrapper for MPI_Alltoallv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* sdispls, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* rdispls, MPI_Datatype recvtype, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            recvcount = 0, recvsz, sendsz, N, i, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Comm_size( comm, &N );
        PMPI_Type_size( recvtype, &recvsz );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            PMPI_Type_size( sendtype, &sendsz );
            for ( i = 0; i < N; i++ )
            {
                recvbytes += recvcounts[ i ] * recvsz;
                sendbytes += sendcounts[ i ] * sendsz;
            }
        }
      #if HAVE( MPI_IN_PLACE )
        else
        {
            PMPI_Comm_rank( comm, &me );
            for ( i = 0; i < N; i++ )
            {
                recvcount += recvcounts[ i ];
            }

            recvcount -= recvcounts[ me ];

            sendbytes = recvbytes = recvcount * recvsz;
        }
      #endif

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLV ] );

        return_val = PMPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLV ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLTOALLV,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALLW ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallw )
/**
 * Measurement wrapper for MPI_Alltoallw
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallw( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int sendcounts[], SCOREP_MPI_CONST_DECL int sdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype sendtypes[], void* recvbuf, SCOREP_MPI_CONST_DECL int recvcounts[], SCOREP_MPI_CONST_DECL int rdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype recvtypes[], MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            recvsz, sendsz, N, i, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Comm_size( comm, &N );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            for ( i = 0; i < N; i++ )
            {
                PMPI_Type_size( recvtypes[ i ], &recvsz );
                recvbytes += recvcounts[ i ] * recvsz;

                PMPI_Type_size( sendtypes[ i ], &sendsz );
                sendbytes += sendcounts[ i ] * sendsz;
            }
        }
      #if HAVE( MPI_IN_PLACE )
        else
        {
            PMPI_Comm_rank( comm, &me );

            for ( i = 0; i < N; i++ )
            {
                PMPI_Type_size( recvtypes[ i ], &recvsz );
                recvbytes += recvcounts[ i ] * recvsz;
            }

            PMPI_Type_size( recvtypes[ me ], &recvsz );
            recvbytes -= recvcounts[ me ] * recvsz;

            sendbytes = recvbytes;
        }
      #endif

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLW ] );

        return_val = PMPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLW ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_ALLTOALLW,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_BARRIER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Barrier )
/**
 * Measurement wrapper for MPI_Barrier
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Barrier( MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        SCOREP_MpiRank root_loc = SCOREP_INVALID_ROOT_RANK;
        SCOREP_MPI_EVENT_GEN_OFF();
        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_BARRIER ] );

        return_val = PMPI_Barrier( comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Barrier( comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_BARRIER ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_BARRIER,
                                 0,
                                 0 );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Barrier( comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_BCAST ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Bcast )
/**
 * Measurement wrapper for MPI_Bcast
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Bcast( void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t        sz, N, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
        }
        else
        {
            N = 0;
        }

        sendbytes = N * count * sz;
        recvbytes = count * sz;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_BCAST ] );

        return_val = PMPI_Bcast( buffer, count, datatype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Bcast( buffer, count, datatype, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_BCAST ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_BCAST,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Bcast( buffer, count, datatype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_EXSCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Exscan )
/**
 * Measurement wrapper for MPI_Exscan
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Exscan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t        sz, me, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

        sendbytes = ( N - me - 1 ) * sz * count;
        recvbytes = me * sz * count;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_EXSCAN ] );

        return_val = PMPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_EXSCAN ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_EXSCAN,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_GATHER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gather )
/**
 * Measurement wrapper for MPI_Gather
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sendsz, recvsz, N, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            PMPI_Type_size( sendtype, &sendsz );
            sendbytes = sendcount * sendsz;
        }
        /* MPI_IN_PLACE: sendbytes is initialized to 0 */

        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );
        #if HAVE( MPI_IN_PLACE )
            if ( sendbuf == MPI_IN_PLACE )
            {
                --N;
            }
        #endif
            recvbytes = recvcount * N * recvsz;
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_GATHER ] );

        return_val = PMPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_GATHER ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_GATHER,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_GATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gatherv )
/**
 * Measurement wrapper for MPI_Gatherv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            recvsz, sendsz, me, N, i;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            PMPI_Type_size( sendtype, &sendsz );
            sendbytes = sendcount * sendsz;
        }
        /* MPI_IN_PLACE: sendbytes is initialized to 0 */

        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );

            for ( i = 0; i < N; ++i )
            {
                recvbytes += recvcounts[ i ] * recvsz;
            }

        #if HAVE( MPI_IN_PLACE )
            if ( sendbuf == MPI_IN_PLACE )
            {
                recvbytes -= recvcounts[ me ] * recvsz;
            }
        #endif
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_GATHERV ] );

        return_val = PMPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_GATHERV ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_GATHERV,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce )
/**
 * Measurement wrapper for MPI_Reduce
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sz, me, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf != MPI_IN_PLACE )
      #endif
        {
            sendbytes = count * sz;
        }
      #if HAVE( MPI_IN_PLACE )
        else
      #endif
        {
            --N;
        }

        if ( root == me )
        {
            recvbytes = N * count * sz;
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_REDUCE ] );

        return_val = PMPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_REDUCE ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_REDUCE,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter )
/**
 * Measurement wrapper for MPI_Reduce_scatter
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            i, sz, me, N, count = 0;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

        for ( i = 0; i < N; i++ )
        {
            count += recvcounts[ i ];
        }

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            sendbytes = ( count - 1 ) * sz;
            recvbytes = ( N - 1 ) * recvcounts[ me ] * sz;
        }
        else
      #endif
        {
            sendbytes = count * sz;
            recvbytes = N * recvcounts[ me ] * sz;
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER ] );

        return_val = PMPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_REDUCE_SCATTER,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE_SCATTER_BLOCK ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter_block )
/**
 * Measurement wrapper for MPI_Reduce_scatter_block
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter_block( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int recvcount, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sz, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_size( comm, &N );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            --N;
        }
      #endif

        sendbytes = N * recvcount * sz;
        recvbytes = sendbytes;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER_BLOCK ] );

        return_val = PMPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER_BLOCK ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_REDUCE_SCATTER_BLOCK,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scan )
/**
 * Measurement wrapper for MPI_Scan
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sz, me, N;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

      #if HAVE( MPI_IN_PLACE )
        if ( sendbuf == MPI_IN_PLACE )
        {
            sendbytes = ( N - me - 1 ) * count * sz;
            recvbytes = me * count * sz;
        }
        else
      #endif
        {
            sendbytes = ( N - me ) * count * sz;
            recvbytes = ( me + 1 ) * count * sz;
        }

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_SCAN ] );

        return_val = PMPI_Scan( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scan( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_SCAN ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_SCAN,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scan( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatter )
/**
 * Measurement wrapper for MPI_Scatter
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatter( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sendsz, recvsz, N, me;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

        PMPI_Comm_rank( comm, &me );

      #if HAVE( MPI_IN_PLACE )
        if ( recvbuf != MPI_IN_PLACE )
      #endif
        {
            if ( me == root )
            {
                PMPI_Comm_size( comm, &N );
                PMPI_Type_size( sendtype, &sendsz );
                sendbytes = N * sendcount * sendsz;
            }

            PMPI_Type_size( recvtype, &recvsz );
            recvbytes = recvcount * recvsz;
        }
      #if HAVE( MPI_IN_PLACE )
        else
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( sendtype, &sendsz );
            sendbytes = ( N - 1 ) * sendcount * sendsz;
            /* recvbytes is initialized to 0 */
        }
      #endif

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_SCATTER ] );

        return_val = PMPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_SCATTER ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_SCATTER,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCATTERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatterv )
/**
 * Measurement wrapper for MPI_Scatterv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatterv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int            sendcount, sendsz, recvsz, me, N, i;
        uint64_t       sendbytes = 0, recvbytes = 0;
        SCOREP_MpiRank root_loc  = root;

        SCOREP_MPI_EVENT_GEN_OFF();

        sendcount = sendsz = 0;
      #if HAVE( MPI_IN_PLACE )
        if ( recvbuf != MPI_IN_PLACE )
      #endif
        {
            PMPI_Type_size( recvtype, &recvsz );
            recvbytes = recvcount * recvsz;
        }
        /* MPI_IN_PLACE: recvbytes is initialized to 0 */

        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( sendtype, &sendsz );
            for ( i = 0; i < N; i++ )
            {
                sendcount += sendcounts[ i ];
            }

        #if HAVE( MPI_IN_PLACE )
            if ( recvbuf == MPI_IN_PLACE )
            {
                sendcount -= sendcounts[ me ];
            }
        #endif
        }
        sendbytes = sendcount * sendsz;

        /* Enters region too. */
        uint64_t start_time_stamp
            = SCOREP_MpiCollectiveBegin( scorep_mpi_regid[ SCOREP__MPI_SCATTERV ] );

        return_val = PMPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif

        /* Leaves region too. */
        SCOREP_MpiCollectiveEnd( scorep_mpi_regid[ SCOREP__MPI_SCATTERV ],
                                 SCOREP_MPI_COMM_HANDLE( comm ),
                                 root_loc,
                                 SCOREP_COLLECTIVE_MPI_SCATTERV,
                                 sendbytes,
                                 recvbytes );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_REDUCE_LOCAL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_local )
/**
 * Measurement wrapper for MPI_Reduce_local
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Reduce_local call with enter and exit events.
 */
int
MPI_Reduce_local( SCOREP_MPI_CONST_DECL void* inbuf, void* inoutbuf, int count, MPI_Datatype datatype, MPI_Op op )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_LOCAL ] );

        return_val = PMPI_Reduce_local( inbuf, inoutbuf, count, datatype, op );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_LOCAL ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_local( inbuf, inoutbuf, count, datatype, op );
    }

    return return_val;
}
#endif

/**
 * @}
 */
