/**
 * @file
 * @ingroup    MPI_Wrapper
 *
 * @brief Registration of MPI functions
 * For all MPI functions a region is registered at initialization time of the MPI adapter.
 * The dynamic region handle obtained from the measurement system is stored in an array
 * unter a fixed index for every region.
 */

#ifndef SCOREP_MPIWRAP_REG_H
#define SCOREP_MPIWRAP_REG_H

#include <SCOREP_Definitions.h>

/*
 * -----------------------------------------------------------------------------
 *
 *  - Registration of MPI functions
 *
 * -----------------------------------------------------------------------------
 */

/** function type is not point-to-point */
#define SCOREP_MPI_TYPE__NONE                          0
/** function type is receive operation */
#define SCOREP_MPI_TYPE__RECV                          1
/** function type is send operation */
#define SCOREP_MPI_TYPE__SEND                          2
/** function type is send and receive operation */
#define SCOREP_MPI_TYPE__SENDRECV                      3
/** function type is collective */
#define SCOREP_MPI_TYPE__COLLECTIVE                    4

/** function has is unknown communication pattern */
#define SCOREP_COLL_TYPE__UNKNOWN                      1
/** function is barrier-like operation */
#define SCOREP_COLL_TYPE__BARRIER                      2
/** function has 1:n communication pattern */
#define SCOREP_COLL_TYPE__ONE2ALL                      3
/** function has n:1 communication pattern */
#define SCOREP_COLL_TYPE__ALL2ONE                      4
/** function has n:n communication pattern */
#define SCOREP_COLL_TYPE__ALL2ALL                      5
/** function may be partially synchronizing */
#define SCOREP_COLL_TYPE__PARTIAL                      6
/** function is implicitely synchronizing */
#define SCOREP_COLL_TYPE__IMPLIED                      7

/** Mappings from MPI collective names to Score-P @{ */
#define SCOREP_MPI_COLLECTIVE__MPI_BARRIER              SCOREP_COLLECTIVE_BARRIER
#define SCOREP_MPI_COLLECTIVE__MPI_BCAST                SCOREP_COLLECTIVE_BROADCAST
#define SCOREP_MPI_COLLECTIVE__MPI_GATHER               SCOREP_COLLECTIVE_GATHER
#define SCOREP_MPI_COLLECTIVE__MPI_GATHERV              SCOREP_COLLECTIVE_GATHERV
#define SCOREP_MPI_COLLECTIVE__MPI_SCATTER              SCOREP_COLLECTIVE_SCATTER
#define SCOREP_MPI_COLLECTIVE__MPI_SCATTERV             SCOREP_COLLECTIVE_SCATTERV
#define SCOREP_MPI_COLLECTIVE__MPI_ALLGATHER            SCOREP_COLLECTIVE_ALLGATHER
#define SCOREP_MPI_COLLECTIVE__MPI_ALLGATHERV           SCOREP_COLLECTIVE_ALLGATHERV
#define SCOREP_MPI_COLLECTIVE__MPI_ALLTOALL             SCOREP_COLLECTIVE_ALLTOALL
#define SCOREP_MPI_COLLECTIVE__MPI_ALLTOALLV            SCOREP_COLLECTIVE_ALLTOALLV
#define SCOREP_MPI_COLLECTIVE__MPI_ALLTOALLW            SCOREP_COLLECTIVE_ALLTOALLW
#define SCOREP_MPI_COLLECTIVE__MPI_ALLREDUCE            SCOREP_COLLECTIVE_ALLREDUCE
#define SCOREP_MPI_COLLECTIVE__MPI_REDUCE               SCOREP_COLLECTIVE_REDUCE
#define SCOREP_MPI_COLLECTIVE__MPI_REDUCE_SCATTER       SCOREP_COLLECTIVE_REDUCE_SCATTER
#define SCOREP_MPI_COLLECTIVE__MPI_REDUCE_SCATTER_BLOCK SCOREP_COLLECTIVE_REDUCE_SCATTER_BLOCK
#define SCOREP_MPI_COLLECTIVE__MPI_SCAN                 SCOREP_COLLECTIVE_SCAN
#define SCOREP_MPI_COLLECTIVE__MPI_EXSCAN               SCOREP_COLLECTIVE_EXSCAN
/** @} */

/**
 * Bitpatterns for runtime wrapper enabling
 */
enum scorep_mpi_groups {
    /* pure groups, which can be specified in conf */
    SCOREP_MPI_ENABLED_CG        =     1,
    SCOREP_MPI_ENABLED_COLL      =     2,
    SCOREP_MPI_ENABLED_ENV       =     4,
    SCOREP_MPI_ENABLED_ERR       =     8,
    SCOREP_MPI_ENABLED_EXT       =    16,
    SCOREP_MPI_ENABLED_IO        =    32,
    SCOREP_MPI_ENABLED_MISC      =    64,
    SCOREP_MPI_ENABLED_P2P       =   128,
    SCOREP_MPI_ENABLED_RMA       =   256,
    SCOREP_MPI_ENABLED_SPAWN     =   512,
    SCOREP_MPI_ENABLED_TOPO      =  1024,
    SCOREP_MPI_ENABLED_TYPE      =  2048,
    SCOREP_MPI_ENABLED_PERF      =  4096,
    SCOREP_MPI_ENABLED_XNONBLOCK =  8192,
    SCOREP_MPI_ENABLED_XREQTEST  = 16384,
    /* derived groups, which are a combination of existing groups */
    SCOREP_MPI_ENABLED_CG_ERR    = SCOREP_MPI_ENABLED_CG    | SCOREP_MPI_ENABLED_ERR,
    SCOREP_MPI_ENABLED_CG_EXT    = SCOREP_MPI_ENABLED_CG    | SCOREP_MPI_ENABLED_EXT,
    SCOREP_MPI_ENABLED_CG_MISC   = SCOREP_MPI_ENABLED_CG    | SCOREP_MPI_ENABLED_MISC,
    SCOREP_MPI_ENABLED_IO_ERR    = SCOREP_MPI_ENABLED_IO    | SCOREP_MPI_ENABLED_ERR,
    SCOREP_MPI_ENABLED_IO_MISC   = SCOREP_MPI_ENABLED_IO    | SCOREP_MPI_ENABLED_MISC,
    SCOREP_MPI_ENABLED_RMA_ERR   = SCOREP_MPI_ENABLED_RMA   | SCOREP_MPI_ENABLED_ERR,
    SCOREP_MPI_ENABLED_RMA_EXT   = SCOREP_MPI_ENABLED_RMA   | SCOREP_MPI_ENABLED_EXT,
    SCOREP_MPI_ENABLED_RMA_MISC  = SCOREP_MPI_ENABLED_RMA   | SCOREP_MPI_ENABLED_MISC,
    SCOREP_MPI_ENABLED_TYPE_EXT  = SCOREP_MPI_ENABLED_TYPE  | SCOREP_MPI_ENABLED_EXT,
    SCOREP_MPI_ENABLED_TYPE_MISC = SCOREP_MPI_ENABLED_TYPE  | SCOREP_MPI_ENABLED_MISC,
    /* NOTE: ALL should comprise all pure groups */
    SCOREP_MPI_ENABLED_ALL       = SCOREP_MPI_ENABLED_CG        |
                                   SCOREP_MPI_ENABLED_COLL      |
                                   SCOREP_MPI_ENABLED_ENV       |
                                   SCOREP_MPI_ENABLED_ERR       |
                                   SCOREP_MPI_ENABLED_EXT       |
                                   SCOREP_MPI_ENABLED_IO        |
                                   SCOREP_MPI_ENABLED_MISC      |
                                   SCOREP_MPI_ENABLED_P2P       |
                                   SCOREP_MPI_ENABLED_RMA       |
                                   SCOREP_MPI_ENABLED_SPAWN     |
                                   SCOREP_MPI_ENABLED_TOPO      |
                                   SCOREP_MPI_ENABLED_TYPE      |
                                   SCOREP_MPI_ENABLED_PERF      |
                                   SCOREP_MPI_ENABLED_XNONBLOCK |
                                   SCOREP_MPI_ENABLED_XREQTEST,
    /* NOTE: DEFAULT should reflect the default set */
    SCOREP_MPI_ENABLED_DEFAULT   = SCOREP_MPI_ENABLED_CG    |
                                   SCOREP_MPI_ENABLED_COLL  |
                                   SCOREP_MPI_ENABLED_ENV   |
                                   SCOREP_MPI_ENABLED_IO    |
                                   SCOREP_MPI_ENABLED_P2P   |
                                   SCOREP_MPI_ENABLED_RMA   |
                                   SCOREP_MPI_ENABLED_TOPO  |
                                   SCOREP_MPI_ENABLED_XNONBLOCK
};

/** Bit vector for runtime measurement wrapper enabling/disabling */
extern uint64_t scorep_mpi_enabled;

enum scorep_mpi_regions
{
#pragma wrapgen multiple restrict() skel/SCOREP_Mpi_RegH.w

    /** Artificial root for MPI-only experiments when no user-code
     * instrumenation is available */
    SCOREP_MPI_REGION__PARALLEL,

    /** Total number of regions */
    SCOREP_MPI_NUM_REGIONS
};

/** Region IDs of MPI functions */
extern SCOREP_RegionHandle scorep_mpi_regions[SCOREP_MPI_NUM_REGIONS];

/**
 * Register MPI functions and initialize data structures
 */
void scorep_mpi_register_regions( void );

#endif /* SCOREP_MPIWRAP_REG_H */
